{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568081f1",
   "metadata": {},
   "source": [
    "## Analise da base de dados `Beta` utilizando algoritmos de ML\n",
    "\n",
    "Neste notebook será analisado o `Beta dataset` utilizando algoritmos de ML para realizar a: \n",
    "\n",
    "1. Extração de características;\n",
    "2. Seleção de características;\n",
    "3. Classificação dos dados.\n",
    "\n",
    "### Pontos importantes do dataset\n",
    "\n",
    "- Frequências estimuladas (total de 40, com diferença de 0.2Hz uma da outra): 8.0; 8.2;...; 15.6; 15.8;\n",
    "- Taxa de amostragem: 250Hz\n",
    "\n",
    "### Analisar os \"momentos\" em que ocorrem evocação do sinal SSVEP\n",
    "1. Criar o objeto `MNE` a partir dos dados do participante;\n",
    "2. Aplicar no objeto `MNE` o filtro passa-faixa nos valores de 6 a 18 Hz;\n",
    "3. Criar cópias do objeto `MNE` com fatias de tempo menores para analisar momentos que ocorrem estimulos ou não (verificar artigo);\n",
    "\n",
    "> 0.0 - 0.5 segundos e 2.5 - 3.0 segundos, ocorre apenas ruído; <br/>\n",
    " 0.5 - 2.5 segundos, ocorre sinal SSVEP (com ruídos)\n",
    " \n",
    "4. Com os sinais separados em objetos `MNE`, aplicar a `FFT`, para que seja possível plotar gráficos que contenham (ou não) as informações:\n",
    "- Os dados devem ser plotados do domínio da frequência (após a transformada de Fourier). O `FFT` pode ser realizado pela biblioteca `scipy.fft`.\n",
    "- Deve ser observado que as janelas (a) com ruído não aparecerão de fato o sinal SSVEP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "541d82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import matplotlib as plt\n",
    "import mne\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# beta = loadmat(f\"../../datasets/beta/S13.mat\")\n",
    "beta = np.load(f\"../../datasets/beta/data_beta.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a79272",
   "metadata": {},
   "source": [
    "### Extração de características\n",
    "\n",
    "Uma característica importante de acordo com o artigo base do dataset `Beta` é o *signal-to-noise ratio* (SNR).\n",
    "\n",
    "> Incluir equação SNR\n",
    "\n",
    "Ao finalizar essa etapa, será obtido um vetor de características. Essas podem ser:\n",
    "- `SNR` (obrigatória);\n",
    "- Maior valor espectral (FFT em vez do compute_psd);\n",
    "- Média dos valores espectrais (FFT).\n",
    "\n",
    "Dimensionalidade dos dados será explicada da seguinte forma:\n",
    "- `40, 4, 64, 750` -> 40 targets, 4 trials, 64 canais, 750 valores \n",
    "- `160, 64 (SNR) + 64 (media) + 64 (maior) ...` Resultando em `160, 192` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9359f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/mne_data_beta.fif ...\n",
      "Isotrak not found\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2996.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>10.0: 4<br/>10.2: 4<br/>10.4: 4<br/>10.6: 4<br/>10.8: 4<br/>11.0: 4<br/>11.2: 4<br/>11.4: 4<br/>11.6: 4<br/>11.8: 4<br/>12.0: 4<br/>12.2: 4<br/>12.4: 4<br/>12.600000000000001: 4<br/>12.8: 4<br/>13.0: 4<br/>13.200000000000001: 4<br/>13.4: 4<br/>13.600000000000001: 4<br/>13.8: 4<br/>14.0: 4<br/>14.200000000000001: 4<br/>14.4: 4<br/>14.600000000000001: 4<br/>14.8: 4<br/>15.0: 4<br/>15.200000000000001: 4<br/>15.4: 4<br/>15.600000000000001: 4<br/>15.8: 4<br/>8.0: 4<br/>8.2: 4<br/>8.4: 4<br/>8.6: 4<br/>8.799999999999999: 4<br/>9.0: 4<br/>9.2: 4<br/>9.4: 4<br/>9.6: 4<br/>9.8: 4</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>0.000 – 2.996 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsFIF |  160 events (all good), 0 – 2.996 s, baseline off, ~58.7 MB, data loaded,\n",
       " '8.0': 4\n",
       " '8.2': 4\n",
       " '8.4': 4\n",
       " '8.6': 4\n",
       " '8.799999999999999': 4\n",
       " '9.0': 4\n",
       " '9.2': 4\n",
       " '9.4': 4\n",
       " '9.6': 4\n",
       " '9.8': 4\n",
       " and 30 more events ...>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = beta['data'][0][0][0]\n",
    "# y = beta['data'][0][0][1][0][0][4][0]\n",
    "\n",
    "# X_beta = np.array(np.transpose(X, (3, 2, 0, 1)))\n",
    "# print(X_beta.shape)\n",
    "\n",
    "# X_beta = X_beta.reshape(X_beta.shape[0] * X_beta.shape[1], X_beta.shape[2], X_beta.shape[3])\n",
    "# print(X_beta.shape)\n",
    "\n",
    "# print(\"\\nLabels:\")\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "\n",
    "\n",
    "mne_data = mne.read_epochs(\"mne_data_beta.fif\")\n",
    "mne_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f392046",
   "metadata": {},
   "source": [
    "> Agora iremos estimar o ruído de fundo, para calcular posteriormente o `narrow SNR` e o `wide-band SNR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6688b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estimando o ruído de fundo\n",
    "\n",
    "# # intervalos de tempo sem estímulo (0 a 0,5 segundos e 2,5 a 3 segundos)\n",
    "# base_start = 0\n",
    "# base_end = 125\n",
    "# rest_start = 625\n",
    "# rest_end = 750\n",
    "\n",
    "# # armazena uma lista com as médias de potência para cada canal\n",
    "# noise_power = []\n",
    "\n",
    "# # consideramos a primeira amostra (1º target, 1º trial)\n",
    "# # for channel_data in X_beta[0, :, :]:\n",
    "# for channel_data in mne_data.get_data()[0, :, :]:\n",
    "#     fft_result = np.fft.fft(channel_data)\n",
    "    \n",
    "#     # densidade espectral de potência (PSD)\n",
    "#     psd = np.abs(fft_result) ** 2\n",
    "    \n",
    "#     # média da potência nos intervalos de tempo sem estímulo\n",
    "#     base_power = np.mean(psd[base_start:base_end])\n",
    "#     rest_power = np.mean(psd[rest_start:rest_end])\n",
    "    \n",
    "#     # média das duas médias de potência obtidas anteriormente\n",
    "#     mean_noise_power = (base_power + rest_power) / 2\n",
    "#     noise_power.append(mean_noise_power)\n",
    "    \n",
    "# #média das médias de potência de todos os canais para estimar o ruído de fundo\n",
    "# estimated_background_noise = np.mean(noise_power)\n",
    "# estimated_background_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95938d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28085.601109550964"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimando o ruído de fundo\n",
    "\n",
    "# intervalos de tempo sem estímulo (0 a 0,5 segundos e 2,5 a 3 segundos)\n",
    "ini_ruido = mne_data.copy().crop(tmin=0.0, tmax=0.5)\n",
    "meio_ruido = mne_data.copy().crop(tmin=0.5, tmax=2.5)\n",
    "fim_ruido = mne_data.copy().crop(tmin=2.5, tmax=3.0)\n",
    "\n",
    "# consideramos a primeira amostra (1º target, 1º trial)\n",
    "fft_ini_result = np.fft.fft(ini_ruido)\n",
    "fft_fim_result = np.fft.fft(fim_ruido)\n",
    "\n",
    "# densidade espectral de potência (PSD)\n",
    "psd_ini = np.abs(fft_ini_result) ** 2\n",
    "psd_fim = np.abs(fft_fim_result) ** 2\n",
    "\n",
    "# média da potência nos intervalos de tempo sem estímulo\n",
    "base_power = np.mean(psd_ini, axis=-1)\n",
    "rest_power = np.mean(psd_fim, axis=-1)\n",
    "\n",
    "# média das duas médias de potência obtidas anteriormente\n",
    "mean_noise_power = np.mean([base_power + rest_power])\n",
    "    \n",
    "#média das médias de potência de todos os canais para estimar o ruído de fundo\n",
    "mean_noise_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015c681",
   "metadata": {},
   "source": [
    "> Antes de calcular os SNRs, precisamos obter as amplitudes alvo por meio dos dados EEG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c99bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.signal import find_peaks\n",
    "\n",
    "# sr = 250\n",
    "\n",
    "# # frequências alvo\n",
    "# target_frequencies = np.arange(8, 16, 0.2)\n",
    "\n",
    "# # lista para armazenar as amplitudes nas frequências alvo\n",
    "# target_amplitudes = []\n",
    "\n",
    "# # for channel_data in X_beta[0, :, :]:\n",
    "# for channel_data in mne_data.get_data():\n",
    "#     fft_result = np.fft.fft(channel_data)\n",
    "#     psd = np.abs(fft_result) ** 2\n",
    "#     frequencies = np.fft.fftfreq(len(fft_result), 1 / sr)\n",
    "#     target_amplitudes_trial = []\n",
    "#     for target_frequency in target_frequencies:\n",
    "#         # encontrando o índice da frequência alvo no espectro de frequência\n",
    "#         index = np.argmin(np.abs(frequencies - target_frequency))\n",
    "#         # amplitude na frequência alvo\n",
    "#         amplitude = np.sqrt(psd[index])\n",
    "#         target_amplitudes_trial.append(amplitude)\n",
    "        \n",
    "#     target_amplitudes.append(target_amplitudes_trial)\n",
    "    \n",
    "# target_amplitudes = np.array(target_amplitudes)\n",
    "# target_amplitudes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b775611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 64, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "sr = 250\n",
    "\n",
    "# frequências alvo\n",
    "target_frequencies = np.arange(8, 16, 0.2)\n",
    "\n",
    "# lista para armazenar as amplitudes nas frequências alvo\n",
    "target_amplitudes = []\n",
    "\n",
    "# for channel_data in X_beta[0, :, :]:\n",
    "for channel_data in mne_data.get_data():\n",
    "    target = []\n",
    "    for eletrodo in channel_data:\n",
    "        # fft_result = np.fft.fft(channel_data)\n",
    "        fft_result = np.fft.fft(eletrodo)\n",
    "        psd = np.abs(fft_result) ** 2\n",
    "        frequencies = np.fft.fftfreq(len(fft_result), 1 / sr)\n",
    "        target_amplitudes_trial = []\n",
    "        for target_frequency in target_frequencies:\n",
    "            # encontrando o índice da frequência alvo no espectro de frequência\n",
    "            index = np.argmin(np.abs(frequencies - target_frequency))\n",
    "            # amplitude na frequência alvo\n",
    "            amplitude = np.sqrt(psd[index])\n",
    "            target_amplitudes_trial.append(amplitude)\n",
    "        \n",
    "        target.append(target_amplitudes_trial)\n",
    "\n",
    "    target_amplitudes.append(target)\n",
    "    \n",
    "target_amplitudes = np.array(target_amplitudes)\n",
    "target_amplitudes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03674497",
   "metadata": {},
   "source": [
    "Vamos calcular o SNR de \"banda estreita\". Pode ser observado pela seguinte equação:\n",
    "\n",
    "$SNR_{banda\\ estreita} = 10 \\cdot \\log_{10}\\left(\\frac{\\text{energia total do espectro}}{\\text{média das amplitudes nas frequências vizinhas}}\\right)$\n",
    "\n",
    "Já o SNR de banda larga é definido da seguinte forma:\n",
    "\n",
    "$SNR_{banda\\ larga} = 10 \\cdot \\log_{10}\\left(\\frac{\\text{energia total do espectro}}{\\text{energia total do espectro de amplitude}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb75707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 64, 40)\n",
      "(160, 64, 40)\n",
      "\n",
      "(160, 64, 80)\n"
     ]
    }
   ],
   "source": [
    "# forçando (estragando) valor de \"estimated_background_noise\" para não sobrar valores negativos\n",
    "estimated_background_noise = 1.\n",
    "target_amplitudes_adjusted = target_amplitudes - estimated_background_noise\n",
    "\n",
    "# subtraindo o ruído de fundo das amplitudes\n",
    "narrow_band_SNR = 10 * np.log10(target_amplitudes_adjusted / estimated_background_noise)\n",
    "# print(narrow_band_SNR)\n",
    "print(narrow_band_SNR.shape)\n",
    "\n",
    "total_power = np.sum(target_amplitudes_adjusted)\n",
    "wide_band_SNR = 10 * np.log10(target_amplitudes_adjusted / total_power)\n",
    "# print(wide_band_SNR)\n",
    "print(wide_band_SNR.shape)\n",
    "\n",
    "\n",
    "X = np.array([narrow_band_SNR, wide_band_SNR])\n",
    "X = X.swapaxes(0, 1)\n",
    "X = X.swapaxes(1, 2)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2] * X.shape[3])\n",
    "print()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1709d",
   "metadata": {},
   "source": [
    "Ao final desta etapa, será obtido um vetor de características. Estas podem ser:\n",
    "- `narrow SNR` (obrigatória);\n",
    "- `wide-band SNR` (obrigatória);\n",
    "- Maior valor espectral (FFT);\n",
    "- Média dos valores espectrais (FFT).\n",
    "\n",
    "> Dimensionalidade dos dados será explicada da seguinte forma:\n",
    "\n",
    "`40, 4, 64, 750` -> 40 targets, 4 trials, 64 canais e 750 valores `160, 64 (SNR) + 64 (média) + 64 (maior) ...`\n",
    "Resultando em `(160, 192)`.\n",
    "\n",
    "`(160, 64, 750)` = 2 * `(160, 64, 40)` = `(160, 64, 80)` => (reshape) `(160, 64)`\n",
    "\n",
    "reshape => `(160, 64, 80)`\n",
    "\n",
    "> (1) **Seleção de características (MANUAL)**\n",
    "\n",
    "Remover os eletrodos não listados no artigo => `(160, 9, 80)` => reshape `(160, 720)`\n",
    "\n",
    "> (2) Vetor de característica para a **seleção de características RFE**\n",
    "\n",
    "`(160, 64, 80)` => reshape => `(160, 5120)`. Após aplicar o RFE, retorna  `(160, N)`\n",
    "\n",
    "> (3) Aplicar SVM (kernel='linear'), nas duas seleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1bbaa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 9, 80)\n",
      "(160, 720)\n"
     ]
    }
   ],
   "source": [
    "# 1 - Seleção de características (MANUAL)\n",
    "\n",
    "ch = np.load(\"../../datasets/beta/channels.npy\")\n",
    "\n",
    "best_ch = ['PZ', 'PO3', 'PO5', 'PO4', 'PO6', 'POZ', 'O1', 'OZ', 'O2']\n",
    "index_remove = np.where(np.isin(ch, best_ch, invert=True))\n",
    "\n",
    "X_selected_ch = np.delete(X, index_remove, axis=1)\n",
    "print(X_selected_ch.shape)\n",
    "\n",
    "X_selected_ch = X_selected_ch.reshape(X_selected_ch.shape[0], X_selected_ch.shape[1] * X_selected_ch.shape[2])\n",
    "print(X_selected_ch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb5b75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 5120)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Vetor de característica para a seleção de características RFE\n",
    "\n",
    "X_reshape = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "print(X_reshape.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa2933",
   "metadata": {},
   "source": [
    "### Seleção de características e classificação\n",
    "\n",
    "Como existem diversos eletrodos (canais) que não obtém sinal SSVEP, podemos extrair as características que não contribuem para a classificação dos dados.\n",
    "\n",
    "Podemos utilizar o método `RFE` (*Recursive Feature Elimination*) aplicando por meio de `sklearn.feature_selection.RFE`, aprimorando o parâmetro `n_features_to_select` até obter o melhor resultado de classificação.\n",
    "\n",
    "Para a classificação propriamente dita, é considerado o uso do método `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca5474ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m svm_model \u001b[39m=\u001b[39m SVC(kernel\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# svm_model = SVC(kernel='linear')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# X_test = X_test.fillna(X_test.mean())\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/vinicius/Documents/RP_SSVEP/ssvep/src/beta/beta_machine_learning.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m svm_model\u001b[39m.\u001b[39;49mpredict(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    816\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[1;32m    418\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[1;32m    432\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    608\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[0;32m--> 611\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    612\u001b[0m         X,\n\u001b[1;32m    613\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    614\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    615\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    616\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    617\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39misspmatrix(X):\n\u001b[1;32m    621\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[1;32m    960\u001b[0m             array,\n\u001b[1;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    964\u001b[0m         )\n\u001b[1;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    125\u001b[0m     X,\n\u001b[1;32m    126\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[1;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[1;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    131\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "X = StandardScaler().fit_transform(X_selected_ch)\n",
    "labels = np.load(\"../../datasets/beta/labels_beta.npy\")\n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "# X_best_ch = X_selected_ch.copy()\n",
    "# y = np.load(\"../../datasets/beta/labels_beta.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# X_best_ch.shape\n",
    "\n",
    "svm_model = SVC(kernel='linear').fit(X_train, y_train)\n",
    "# svm_model = SVC(kernel='linear')\n",
    "\n",
    "# X_test = X_test.fillna(X_test.mean())\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fd3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = SVR(kernel='linear')\n",
    "# selector = RFE(estimator, n_features_to_select=5)\n",
    "# selector = selector.fit(X_reshape.shape[0], )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
